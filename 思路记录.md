模型：
/shared_home/arknet/hf_models/TinyLlama-1.1B-Chat-v1.0-x2-MoE 这个看起来也比较靠谱
/shared_home/arknet/hf_models/LLaMA-MoE-v1-3_0B-2_16 这个看上去比较靠谱 
/shared_home/arknet/hf_models/switch-base-8
https://huggingface.co/models?sort=downloads&search=moe+1b

性能记录：
```csv
name, mmlu_local_generate,gsm8k, MBPP 
tiny_llama,0,0,0
```


loss感觉还是应该使用fuselm的loss
先试试Continue Pretrain的效果
数据就使用distilling step by step的CoT数据吧
lora从svd得到？
其是把CoT过程作为一个task，但是这里可能还是作为辅助输入比较好
gemma

1. 先跑评测

2. 先跑一个simple average

3. 几种方法：
  - 保留一个专家+7个lora
  - 一个专家+8个lora
  - 全部lora
  - 只保留一个linear，原本一个top2mlp包含3个linear 提升太小了？
  - 权重清洗或者初始化一下
  - 改成soft routing？使用权重加权平均lora 权重 // 她那个是token级别的吗，一个token算一次不会开销很大吗
  - sparse router 的梯度是近似的
  - expert load balance 
  - expert dropout
  - expert的中间层loss
  - 用lora_ab还是三个lora_ab？
  - peft 提供了ties-merge在util中
  - modeling_mixtral中的load_balancing_loss_func
  - mix of vector? dora?
  - 不同的loss function
  - 只记录sparsemoe前后然后使用kl散度是不是就够了？
  - KG&KD?
  - 学习类似jpeg和傅立叶变换的分解？
  - 我记得有篇论文， 从权重角度合并的来着 Dataless Knowledge Fusion by Merging Weights of Language Models（RegMean
  - 看看distillbert
  - 初始化使用模型融合的方法？
  - 先测试一下几种融合方法？
  - label smooth，数据增强等trick
  - 多步，先把三个linear搞成一个

  1. **主成分分析（PCA）**：PCA 通过正交变换将可能相关的变量转换成一组线性不相关的变量，称为主成分。PCA 可以看作是一种特殊的 SVD 应用。

2. **非负矩阵分解（NMF）**：NMF 是一种矩阵分解技术，其中所有矩阵的元素都是非负的。这种约束使得 NMF 特别适合于数据挖掘和特征提取中，特别是在处理图像、文本数据时，因为这些数据自然是非负的。

3. **QR 分解**：QR 分解是将矩阵分解为一个正交矩阵和一个上三角矩阵的乘积。QR 分解在数值线性代数中非常重要，常用于求解线性方程组、计算特征值等。

4. **LU 分解**：LU 分解是将矩阵分解为一个下三角矩阵和一个上三角矩阵的乘积。LU 分解用于解线性方程组、计算矩阵的逆以及计算行列式。

5. **Cholesky 分解**：Cholesky 分解是将一个正定对称矩阵分解为一个下三角矩阵及其转置的乘积。这种分解特别适合于求解某些特殊类型的线性方程组，如正定对称矩阵。

6. **特征值分解（EVD）**：特征值分解是将矩阵分解为其特征向量和特征值。对于每个特征值，有一个相应的特征向量。特征值分解适用于正方形矩阵。

7. **张量分解**：在更高维度上，类似于矩阵分解的技术被用于张量（多维数组）。例如，CP 分解（Candecomp/Parafac）和 TUCKER 分解是两种常见的张量分解方法，它们在信号处理和机器学习中有广泛应用