{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == 128:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = ds_valid.map(\n",
    "    tokenize, batched=True, remove_columns=ds_valid.column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, pad_to_multiple_of=16)\n",
    "out = data_collator([tokenized_datasets[i] for i in range(5)])\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['input_ids'][0][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['labels'][0][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft.utils.config import TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"AIChenKai/TinyLlama-1.1B-Chat-v1.0-x2-MoE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = get_peft_model(\n",
    "    model,\n",
    "    LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        lora_dropout=0,\n",
    "        lora_alpha=1,\n",
    "        target_modules=[\"q_proj\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model(torch.tensor([[1,2]]))['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model(input_ids = torch.tensor([[1,2]]), attention_mask = torch.tensor([[1,1]]), labels = torch.tensor([1,2])).loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(p_model.named_modules())[\"base_model.model.lm_head\"].training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from textbrewer import DistillationConfig, TrainingConfig, GeneralDistiller\n",
    "from peft import EVELoraConfig, TaskType, get_peft_model\n",
    "from merge_methods import keep1\n",
    "\n",
    "import textbrewer\n",
    "import torch\n",
    "\n",
    "eve_config = EVELoraConfig(task_type=TaskType.CAUSAL_LM, merge_method=keep1,\n",
    "                           lora_dropout=0, lora_alpha=0)\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./models/TinyLlama-1.1B-Chat-v1.0-x2-MoE/\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "#student_model = get_peft_model(teacher_model, eve_config)\n",
    "#student_model.print_trainable_parameters()\n",
    "\n",
    "dataset = load_dataset(\"JeanKaddour/minipile\", keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得先tokenization 好然后传进去\n",
    "# callback(model, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.generate(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_adaptor(batch, model_outputs):\n",
    "    # The second and third elements of model outputs are the logits and hidden states\n",
    "    return {'logits': model_outputs[1],\n",
    "            'hidden': model_outputs[2]}\n",
    "\n",
    "\n",
    "train_config = TrainingConfig()\n",
    "# Distillation configuration\n",
    "# Matching different layers of the student and the teacher\n",
    "# We match 0-0 and 8-2 here for demonstration\n",
    "distill_config = DistillationConfig(\n",
    "    hard_label_weight=0.2, temperature=2,\n",
    "    intermediate_matches=[\n",
    "        {'layer_T': 0, 'layer_S': 0, 'feature': 'hidden',\n",
    "            'loss': 'hidden_mse', 'weight': 1},\n",
    "        {'layer_T': 8, 'layer_S': 8, 'feature': 'hidden', \n",
    "         'loss': 'hidden_mse', 'weight': 1}])\n",
    "\n",
    "# Build distiller\n",
    "distiller = GeneralDistiller(\n",
    "    train_config=train_config, distill_config=distill_config,\n",
    "    model_T=teacher_model, model_S=student_model,\n",
    "    adaptor_T=simple_adaptor, adaptor_S=simple_adaptor)\n",
    "\n",
    "# Start!\n",
    "with distiller:\n",
    "    distiller.train(optimizer, dataloader, num_epochs=1,\n",
    "                    callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = AutoModel.from_pretrained(\n",
    "    \"./models/TinyLlama-1.1B-Chat-v1.0-x2-MoE/\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "student_model = get_peft_model(teacher_model, eve_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "load_dataset(\"JeanKaddour/minipile\", split=\"train\", keep_in_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def first_expert(experts: nn.ModuleList, lora_experts: nn.ModuleList):\n",
    "    return experts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration,AutoModel, AutoConfig\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "from peft import get_peft_model, EVELoraConfig, TaskType\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"models/TinyLlama-1.1B-Chat-v1.0-x2-MoE\"\n",
    ")\n",
    "eve_config = EVELoraConfig(task_type=TaskType.CAUSAL_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"models/TinyLlama-1.1B-Chat-v1.0-x2-MoE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, eve_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experts = [\n",
    "        \"expert_0\",\n",
    "        \"expert_1\",\n",
    "        \"expert_2\",\n",
    "        \"expert_3\",\n",
    "        \"expert_4\",\n",
    "        \"expert_5\",\n",
    "        \"expert_6\",\n",
    "        \"expert_7\",\n",
    "    ]\n",
    "def layer_alter(model: nn.Module, alter_func, layer_type) -> None:\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            layer_alter(module, alter_func, layer_type)\n",
    "\n",
    "        if isinstance(module, layer_type):\n",
    "            alter_func(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_module = switch_transformers.SwitchTransformersSparseMLP\n",
    "model_path = \"/shared_home/arknet/hf_models/switch-base-8\"\n",
    "dtype = torch.bfloat16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = SwitchTransformersForConditionalGeneration.from_pretrained(model_path, device_map=\"cuda:2\", torch_dtype=dtype)\n",
    "input_text = \"A <extra_id_0> walks into a bar a orders a <extra_id_1> with <extra_id_2> pinch of <extra_id_3>.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cos(module: nn.Module):\n",
    "    num_experts = len(module.experts)\n",
    "    main_expert = module.experts\n",
    "    \n",
    "    similarity_matrix = torch.zeros((8, 8))\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            similarity_matrix[i, j] = cosine_similarity(\n",
    "                module.experts[experts[i]].wi.weight.view(-1),\n",
    "                module.experts[experts[j]].wi.weight.view(-1),\n",
    "                dim=0,\n",
    "            )\n",
    "    plt.imshow(similarity_matrix.detach().numpy(), cmap='Blues', interpolation='nearest')\n",
    "\n",
    "    # 在每个方块上显示具体的数字\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            plt.text(j, i, format(similarity_matrix[i, j], '.2f'), ha='center', va='center', color='red')\n",
    "\n",
    "    # 设置颜色映射范围\n",
    "    plt.clim(0, 1)\n",
    "\n",
    "    # 添加颜色条\n",
    "    plt.colorbar()\n",
    "\n",
    "    # 设置坐标轴标签和标题\n",
    "    plt.xticks(range(8), experts, rotation=90)\n",
    "    plt.yticks(range(8), experts)\n",
    "    plt.title('Similarity Matrix')\n",
    "\n",
    "    # 显示热力图\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现计划"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用 print_trainable_parameters（） 方法比较 PeftModel 的参数数与基础模型中的参数数！\n",
    "# expert\n",
    "# class SwitchTransformersDenseActDense(nn.Module):\n",
    "#     def __init__(self, config: SwitchTransformersConfig):\n",
    "#         self.wi = nn.Linear(config.d_model, config.d_ff, bias=False)\n",
    "#         self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n",
    "\n",
    "# 这里的门控分数就是一个scale factor 和MEO的思路类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_avg(module: nn.Module):\n",
    "    expert = module.experts['expert_0']\n",
    "    \n",
    "def sparse_first(module: nn.Module):\n",
    "    expert = module.experts['expert_0']\n",
    "    def first_forward(self, hidden_states):\n",
    "        pass\n",
    "def sparse_lora(module:nn.Module):\n",
    "    num_experts = len(module.experts)\n",
    "    main_expert = module.experts\n",
    "    def lora_forward(self, hidden_states):\n",
    "        router_mask, router_probs, router_logits = self.router(hidden_states)\n",
    "        expert_index = torch.argmax(router_mask, dim=-1)\n",
    "        # 这里的mask是一个tokens*num_experts的矩阵，表示每个token应该由哪个专家来处理\n",
    "\n",
    "        next_states = hidden_states.clone()\n",
    "        # 这个for函数，每次处理一个专家对应的所有token\n",
    "        # 计划改成 expert + lora_up*lora_down\n",
    "        # expert(hidden_states[token_indices]).to(next_states.dtype)\n",
    "        for idx, expert in enumerate(self.experts.values()):\n",
    "            token_indices = router_mask[:, :, idx].bool()\n",
    "            next_states[token_indices] = expert(hidden_states[token_indices]).to(next_states.dtype)\n",
    "\n",
    "        hidden_states = router_probs * next_states\n",
    "        return hidden_states, (router_logits, expert_index)\n",
    "    # d_model, d_ff = main_expert.wi.in_features, main_expert.wo.out_features\n",
    "\n",
    "    module = [\n",
    "        \n",
    "    ]\n",
    "    # module.forward = types.MethodType(switch_forward, module)\n",
    "layer_alter(model, sparse_first, sparse_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = torch.zeros((8, 8))\n",
    "plt.imshow(similarity_matrix, cmap=\"hot\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.modules())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_mlp = model.spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
